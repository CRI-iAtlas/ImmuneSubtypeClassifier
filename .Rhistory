devtools::install_github('gibbsdavidl/robencla')
mod$ensbl[[`2`]]
mod$ensbl["2"]]
mod$ensbl[["2"]]
robencla::Ensemble
robencla::Ensemble$initialize
robencla::Ensemble()$initialize
robencla::Ensemble()
robencla::Data_eng()
mod$autocv(
# The data to use for training
data_frame=dat,
# The name of the column containing the training label
label_name='Class',
# The column containing the sample ID
sample_id = 'AliquotBarcode',
# The number of cross validation folds to predict on, ignores data_split if >1.
cv_rounds=5,
# The data transformation mode, set using  allpairs, pairs,sigpairs, quartiles,tertiles,binarize,ranks,original
data_mode=c('namedpairs'),  # pairs takes LR pairs from the pair_list, allpairs forms all pairs from pair_list
# If using signature pairs, then the list defining the signatures, must be composed of column names
signatures=sig_features,
# Use only these features for the pairs
pair_list=pair_features,
# The XGBoost parameters (and a few extra params)
params=params
)
this_deng <- Data_eng$new(mod$data_mode, mod$signatures, mod$pair_list)
mod$data_mode
this_deng <- Data_eng$new(mod$data_mode, mod$signatures, mod$pair_list)
devtools::install_github('gibbsdavidl/robencla',force = T)
devtools::install_github('gibbsdavidl/robencla')
devtools::install_github('gibbsdavidl/robencla',force=T)
library(robencla)
library(readr)
load("E:/Work/GBM_clusters/ImmuneSubtypeClassifier/data/geneSetSymbols.rda")
load("E:/Work/GBM_clusters/ImmuneSubtypeClassifier/data/modelgenes.rda")
load("E:/Work/GBM_clusters/ImmuneSubtypeClassifier/data/ebpp_gene.rda")
label_table <- read_table('inst/data/five_signature_mclust_ensemble_results.tsv.gz')
dat <- read_csv("E:/Work/iatlas/Subtype_Classifier_Training_Dir/data/EBpp_pancancer.csv.gz")
# let's randomize columns with zero variance
idx <- which(apply(dat, 2, var) == 0)
for (i in idx) {
dat[,i] <- rnorm(n=9129)
}
# pick out genes that are in the data
genes <- colnames(dat)[colnames(dat) %in% ebpp_genes_sig$Symbol]
pair_features <- list()
for (ci in 1:6) {
print(ci)
gs <- sample(genes, size = 10, replace = F)
pair_features[[ci]] <- gs
}
sig_features <- list()
for (li in names(genesetsymbols)) {
print(li)
gs <- genesetsymbols[[li]][genesetsymbols[[li]] %in% colnames(dat)]
sig_features[[li]] <- sample(gs, size = 10, replace = F)
}
# XGBoost parameters to pass to each sub-classifier in the ensembles
params <- list(
max_depth=6,    # "height" of the tree, 6 is actually default. I think about 12 seems better.  (xgboost parameter)
eta=0.2,        # this is the learning rate. smaller values slow it down, more conservative   (xgboost parameter)
nrounds=24,     # number of rounds of training, lower numbers less overfitting (potentially)  (xgboost parameter)
early_stopping_rounds=2, # number of rounds without improvment stops the training (xgboost early_stopping_rounds)
nthreads=4,     # parallel threads
gamma=0.0,      # min loss required to again split a leaf node. higher ~ more conservative (xgboost parameter)
lambda=1.0,     # L2 regularization term on weights, higher number ~ more conservative (xgboost parameter)
alpha=0.0,      # L1 regularization term on weights. higher number ~ more conservative (xgboost parameter)
size=11,        # Size of the ensemble, per binary prediction
sample_prop=0.8, # The percentage of data used to train each ensemble member.
feature_prop=0.8, # The percentage of data used to train each ensemble member.
subsample=0.8,   # the xgboost machines subsample at this rate.
combine_function='median',  # How the ensemble should be combined. Only median currently.
verbose=0)
dat$AliquotBarcode <- label_table$AliquotBarcode
dat$Class <- label_table$ClusterModel1
mod <- Robencla$new("Test1")
mod$version
mod$autocv(
# The data to use for training
data_frame=dat,
# The name of the column containing the training label
label_name='Class',
# The column containing the sample ID
sample_id = 'AliquotBarcode',
# The number of cross validation folds to predict on, ignores data_split if >1.
cv_rounds=5,
# The data transformation mode, set using  allpairs, pairs,sigpairs, quartiles,tertiles,binarize,ranks,original
data_mode=c('namedpairs'),  # pairs takes LR pairs from the pair_list, allpairs forms all pairs from pair_list
# If using signature pairs, then the list defining the signatures, must be composed of column names
signatures=sig_features,
# Use only these features for the pairs
pair_list=pair_features,
# The XGBoost parameters (and a few extra params)
params=params
)
mod$autocv(
# The data to use for training
data_frame=dat,
# The name of the column containing the training label
label_name='Class',
# The column containing the sample ID
sample_id = 'AliquotBarcode',
# The number of cross validation folds to predict on, ignores data_split if >1.
cv_rounds=5,
# The data transformation mode, set using  allpairs, pairs,sigpairs, quartiles,tertiles,binarize,ranks,original
data_mode=c('namedpairs'),  # pairs takes LR pairs from the pair_list, allpairs forms all pairs from pair_list
# If using signature pairs, then the list defining the signatures, must be composed of column names
signatures=sig_features,
# Use only these features for the pairs
pair_list=pair_features,
# The XGBoost parameters (and a few extra params)
params=params,
verbose=T
)
dim(mod$data)
mod$pair_list
mod$train(data_table=dat,
label_name='Class',
sample_id = 'AliquotBarcode',
data_mode=c('namedpairs'),
pair_list=pair_features,
signatures=NULL,
params=params)
mod$train(data_table=dat,
label_name='Class',
sample_id = 'AliquotBarcode',
data_mode=c('namedpairs'),
pair_list=pair_features,
signatures=NULL,
params=params)
mod$train(data_frame=dat,
label_name='Class',
sample_id = 'AliquotBarcode',
data_mode=c('namedpairs'),
pair_list=pair_features,
signatures=NULL,
params=params)
unlist(mod$pair_list)
c(unlist(mod$pair_list), unlist(mod$signatures)
)
pair_features
sig_features
mod$pair_list
mod$ensbl[["1"]]
mod$ensbl[["1"]]$pair_list
mod$unique_labels
class(self$pair_list) == "list"
class(mod$pair_list) == "list"
mod$pair_list[["1"]]
mod$pair_list
names(mod$pair_list)
lx <- list()
lx[[1]] <- c(123)
lx[[2]] <- c(3,4,5)
lx
names(lx)
lx[["1"]] <- c(1,2,3)
lx[["2"]] <- c(1,2,3)
names(lx)
lx
mod$pair_list
pair_features
names(pair_features) <- c('1','2','3','4','5','6')
# pick out genes that are in the data
genes <- colnames(dat)[colnames(dat) %in% ebpp_genes_sig$Symbol]
pair_features <- list()
for (ci in 1:6) {
print(ci)
gs <- sample(genes, size = 10, replace = F)
pair_features[[ci]] <- gs
}
names(pair_features) <- c('1','2','3','4','5','6')
sig_features <- list()
for (li in names(genesetsymbols)) {
print(li)
gs <- genesetsymbols[[li]][genesetsymbols[[li]] %in% colnames(dat)]
sig_features[[li]] <- sample(gs, size = 10, replace = F)
}
label_table$ClusterModel1 <- as.character(label_table$ClusterModel1)
mod <- Robencla$new("Test1")
mod$version
mod$autocv(
# The data to use for training
data_frame=dat,
# The name of the column containing the training label
label_name='Class',
# The column containing the sample ID
sample_id = 'AliquotBarcode',
# The number of cross validation folds to predict on, ignores data_split if >1.
cv_rounds=5,
# The data transformation mode, set using  allpairs, pairs,sigpairs, quartiles,tertiles,binarize,ranks,original
data_mode=c('namedpairs'),  # pairs takes LR pairs from the pair_list, allpairs forms all pairs from pair_list
# If using signature pairs, then the list defining the signatures, must be composed of column names
signatures=sig_features,
# Use only these features for the pairs
pair_list=pair_features,
# The XGBoost parameters (and a few extra params)
params=params,
verbose=T
)
res0 <- mod$results()
mod$classification_metrics()
table(Label=mod$test_label, Pred=mod$results(include_label = T)$BestCalls)
mod$importance()
plot_pred_final(mod)
library(robencla)
library(readr)
load("E:/Work/GBM_clusters/ImmuneSubtypeClassifier/data/geneSetSymbols.rda")
load("E:/Work/GBM_clusters/ImmuneSubtypeClassifier/data/modelgenes.rda")
load("E:/Work/GBM_clusters/ImmuneSubtypeClassifier/data/ebpp_gene.rda")
label_table <- read_table('inst/data/five_signature_mclust_ensemble_results.tsv.gz')
label_table$ClusterModel1 <- as.character(label_table$ClusterModel1)
dat <- read_csv("E:/Work/iatlas/Subtype_Classifier_Training_Dir/data/EBpp_pancancer.csv.gz")
# let's randomize columns with zero variance
idx <- which(apply(dat, 2, var) == 0)
for (i in idx) {
dat[,i] <- rnorm(n=9129)
}
# pick out genes that are in the data
genes <- colnames(dat)[colnames(dat) %in% ebpp_genes_sig$Symbol]
pair_features <- list()
for (ci in 1:6) {
print(ci)
gs <- sample(genes, size = 10, replace = F)
pair_features[[ci]] <- gs
}
names(pair_features) <- c('C1','C2','C3','C4','C5','C6')
sig_features <- list()
for (li in names(genesetsymbols)) {
print(li)
gs <- genesetsymbols[[li]][genesetsymbols[[li]] %in% colnames(dat)]
sig_features[[li]] <- sample(gs, size = 10, replace = F)
}
# XGBoost parameters to pass to each sub-classifier in the ensembles
params <- list(
max_depth=12,    # "height" of the tree, 6 is actually default. I think about 12 seems better.  (xgboost parameter)
eta=0.3,        # this is the learning rate. smaller values slow it down, more conservative   (xgboost parameter)
nrounds=64,     # number of rounds of training, lower numbers less overfitting (potentially)  (xgboost parameter)
early_stopping_rounds=2, # number of rounds without improvment stops the training (xgboost early_stopping_rounds)
nthreads=4,     # parallel threads
gamma=0.0,      # min loss required to again split a leaf node. higher ~ more conservative (xgboost parameter)
lambda=1.0,     # L2 regularization term on weights, higher number ~ more conservative (xgboost parameter)
alpha=0.0,      # L1 regularization term on weights. higher number ~ more conservative (xgboost parameter)
size=21,        # Size of the ensemble, per binary prediction
sample_prop=0.8, # The percentage of data used to train each ensemble member.
feature_prop=0.8, # The percentage of data used to train each ensemble member.
subsample=0.8,   # the xgboost machines subsample at this rate.
combine_function='median',  # How the ensemble should be combined. Only median currently.
verbose=0)
#dat$AliquotBarcode <- label_table$AliquotBarcode
dat$Class <- sapply(as.character(label_table$ClusterModel1), FUN = function(A) paste0("C",A))
mod <- Robencla$new("Test1")
mod$version
pair_features
mod$train(data_frame = dat,
label_name = 'Class',
sample_id  = 'Barcode',
data_mode  = 'pairs',
pair_list  = pair_features,
params = params
)
mod$train(data_frame = dat,
label_name = 'Class',
sample_id  = 'Barcode',
data_mode  = 'pairs',
pair_list  = pair_features,
params = params
)
mod$predict(data_frame = dat,
label_name = 'Class',
sample_id  = 'Barcode')
df <- mod$results(include_label = T)
# create a confusion matrix
table(Label=df$Label, Pred=df$BestCalls)
# Prediction metrics on the test set predictions
mod$classification_metrics(use_cv_results = F)
df <- mod$results(include_label = T)
# create a confusion matrix
table(Label=df$Label, Pred=df$BestCalls)
# Prediction metrics on the test set predictions
mod$classification_metrics(use_cv_results = F)
# Get the importance of features in each ensemble member
# these are found by combining the importance measures across the ensemble
mod$importance()
plot_roc(mod, 'C3')
ensemble_rocs(mod)
# Plot the ROC curves for each class
par(mar=c(3,3,3,3))
ensemble_rocs(mod)
plot_roc(mod, 'C3')
plot_pred_final(mod, cluster = T)
mod <- Robencla$new("Test1")
mod$version
mod$train(data_frame = dat,
label_name = 'Class',
sample_id  = 'Barcode',
data_mode  = 'pairs',
pair_list  = pair_features,
params = params
)
mod$predict(data_frame = dat,
label_name = 'Class',
sample_id  = 'Barcode')
df <- mod$results(include_label = T)
# create a confusion matrix
table(Label=df$Label, Pred=df$BestCalls)
# Prediction metrics on the test set predictions
#mod$classification_metrics(use_cv_results = F)
# Get the importance of features in each ensemble member
# these are found by combining the importance measures across the ensemble
#mod$importance()
# Plot the ROC curves for each classpar(mar=c(3,3,3,3))
#plot_roc(mod, 'C3')
#ensemble_rocs(mod)
#plot_pred_final(mod, cluster = T)
# XGBoost parameters to pass to each sub-classifier in the ensembles
params <- list(
max_depth=12,    # "height" of the tree, 6 is actually default. I think about 12 seems better.  (xgboost parameter)
eta=0.3,        # this is the learning rate. smaller values slow it down, more conservative   (xgboost parameter)
nrounds=24,     # number of rounds of training, lower numbers less overfitting (potentially)  (xgboost parameter)
early_stopping_rounds=2, # number of rounds without improvment stops the training (xgboost early_stopping_rounds)
nthreads=4,     # parallel threads
gamma=0.0,      # min loss required to again split a leaf node. higher ~ more conservative (xgboost parameter)
lambda=1.0,     # L2 regularization term on weights, higher number ~ more conservative (xgboost parameter)
alpha=0.0,      # L1 regularization term on weights. higher number ~ more conservative (xgboost parameter)
size=21,        # Size of the ensemble, per binary prediction
sample_prop=0.8, # The percentage of data used to train each ensemble member.
feature_prop=0.8, # The percentage of data used to train each ensemble member.
subsample=0.8,   # the xgboost machines subsample at this rate.
combine_function='median',  # How the ensemble should be combined. Only median currently.
verbose=0)
```{r}
mod <- Robencla$new("Test1")
mod$version
mod$train(data_frame = dat,
label_name = 'Class',
sample_id  = 'Barcode',
data_mode  = 'pairs',
pair_list  = pair_features,
params = params
)
mod$predict(data_frame = dat,
label_name = 'Class',
sample_id  = 'Barcode')
df <- mod$results(include_label = T)
# create a confusion matrix
table(Label=df$Label, Pred=df$BestCalls)
# Prediction metrics on the test set predictions
#mod$classification_metrics(use_cv_results = F)
# Get the importance of features in each ensemble member
# these are found by combining the importance measures across the ensemble
#mod$importance()
# Plot the ROC curves for each classpar(mar=c(3,3,3,3))
#plot_roc(mod, 'C3')
#ensemble_rocs(mod)
#plot_pred_final(mod, cluster = T)
mod$ensbl[[1]]$params
# create a confusion matrix
table(Label=df$Label, Pred=df$BestCalls)
# XGBoost parameters to pass to each sub-classifier in the ensembles
params <- list(
max_depth=6,    # "height" of the tree, 6 is actually default. I think about 12 seems better.  (xgboost parameter)
eta=0.3,        # this is the learning rate. smaller values slow it down, more conservative   (xgboost parameter)
nrounds=24,     # number of rounds of training, lower numbers less overfitting (potentially)  (xgboost parameter)
early_stopping_rounds=2, # number of rounds without improvment stops the training (xgboost early_stopping_rounds)
nthreads=4,     # parallel threads
gamma=0.0,      # min loss required to again split a leaf node. higher ~ more conservative (xgboost parameter)
lambda=1.0,     # L2 regularization term on weights, higher number ~ more conservative (xgboost parameter)
alpha=0.0,      # L1 regularization term on weights. higher number ~ more conservative (xgboost parameter)
size=21,        # Size of the ensemble, per binary prediction
sample_prop=0.8, # The percentage of data used to train each ensemble member.
feature_prop=0.8, # The percentage of data used to train each ensemble member.
subsample=0.8,   # the xgboost machines subsample at this rate.
combine_function='median',  # How the ensemble should be combined. Only median currently.
verbose=0)
mod <- Robencla$new("Test1")
mod$version
mod$train(data_frame = dat,
label_name = 'Class',
sample_id  = 'Barcode',
data_mode  = 'pairs',
pair_list  = pair_features,
params = params
)
mod$predict(data_frame = dat,
label_name = 'Class',
sample_id  = 'Barcode')
df <- mod$results(include_label = T)
# create a confusion matrix
table(Label=df$Label, Pred=df$BestCalls)
# Prediction metrics on the test set predictions
#mod$classification_metrics(use_cv_results = F)
# Get the importance of features in each ensemble member
# these are found by combining the importance measures across the ensemble
#mod$importance()
# Plot the ROC curves for each classpar(mar=c(3,3,3,3))
#plot_roc(mod, 'C3')
#ensemble_rocs(mod)
#plot_pred_final(mod, cluster = T)
# XGBoost parameters to pass to each sub-classifier in the ensembles
params <- list(
max_depth=12,    # "height" of the tree, 6 is actually default. I think about 12 seems better.  (xgboost parameter)
eta=0.3,        # this is the learning rate. smaller values slow it down, more conservative   (xgboost parameter)
nrounds=24,     # number of rounds of training, lower numbers less overfitting (potentially)  (xgboost parameter)
early_stopping_rounds=2, # number of rounds without improvment stops the training (xgboost early_stopping_rounds)
nthreads=4,     # parallel threads
gamma=0.0,      # min loss required to again split a leaf node. higher ~ more conservative (xgboost parameter)
lambda=1.0,     # L2 regularization term on weights, higher number ~ more conservative (xgboost parameter)
alpha=0.0,      # L1 regularization term on weights. higher number ~ more conservative (xgboost parameter)
size=21,        # Size of the ensemble, per binary prediction
sample_prop=0.8, # The percentage of data used to train each ensemble member.
feature_prop=0.8, # The percentage of data used to train each ensemble member.
subsample=0.8,   # the xgboost machines subsample at this rate.
combine_function='median',  # How the ensemble should be combined. Only median currently.
verbose=0)
mod <- Robencla$new("Test1")
mod$version
mod$train(data_frame = dat,
label_name = 'Class',
sample_id  = 'Barcode',
data_mode  = 'pairs',
pair_list  = pair_features,
params = params
)
mod$predict(data_frame = dat,
label_name = 'Class',
sample_id  = 'Barcode')
df <- mod$results(include_label = T)
# create a confusion matrix
table(Label=df$Label, Pred=df$BestCalls)
# Prediction metrics on the test set predictions
#mod$classification_metrics(use_cv_results = F)
# Get the importance of features in each ensemble member
# these are found by combining the importance measures across the ensemble
#mod$importance()
# Plot the ROC curves for each classpar(mar=c(3,3,3,3))
#plot_roc(mod, 'C3')
#ensemble_rocs(mod)
#plot_pred_final(mod, cluster = T)
mod <- Robencla$new("Test1")
mod$version
mod$train(data_frame = dat,
label_name = 'Class',
sample_id  = 'Barcode',
data_mode  = 'named_pairs',
pair_list  = pair_features,
params = params
)
mod$pair_list
gsub(' ', '_', mod$pair_list)
mod$ensbl[[1]]
mod$ensbl[[1]]$pair_list
mod2 <- Robencla$new("Test1")
mod2$version
mod2$train(data_frame = dat,
label_name = 'Class',
sample_id  = 'Barcode',
data_mode  = 'allpairs',
pair_list  = pair_features,
params = params
)
mod2$predict(data_frame = dat,
label_name = 'Class',
sample_id  = 'Barcode')
df <- mod2$results(include_label = T)
# create a confusion matrix
table(Label=df$Label, Pred=df$BestCalls)
# Prediction metrics on the test set predictions
#mod$classification_metrics(use_cv_results = F)
# Get the importance of features in each ensemble member
# these are found by combining the importance measures across the ensemble
#mod$importance()
# Plot the ROC curves for each classpar(mar=c(3,3,3,3))
#plot_roc(mod, 'C3')
#ensemble_rocs(mod)
#plot_pred_final(mod, cluster = T)
header(mod$ensbl[[1]]$train_data)
head(mod$ensbl[[1]]$train_data)
header(mod$ensbl[[1]]$test_data)
head(mod$ensbl[[1]]$test_data)
head(mod2$ensbl[[1]]$test_data)
head(mod$ensbl[[1]]$test_data)
head(mod2$ensbl[[1]]$test_data)
head(mod2$ensbl[[1]]$train_data)
mod <- Robencla$new("Test1")
mod$version
mod$train(data_frame = dat,
label_name = 'Class',
sample_id  = 'Barcode',
data_mode  = 'pairs',
pair_list  = pair_features,
params = params
)
mod$train(data_frame = dat,
label_name = 'Class',
sample_id  = 'Barcode',
data_mode  = 'pairs',
pair_list  = pair_features,
params = params
)
mod$predict(data_frame = dat,
label_name = 'Class',
sample_id  = 'Barcode')
df <- mod$results(include_label = T)
# create a confusion matrix
table(Label=df$Label, Pred=df$BestCalls)
```
colnames(mod$ensbl[[1]]$train_data)
colnames(mod2$ensbl[[1]]$train_data)
colnames(mod2$ensbl[[1]]$train_data) == colnames(mod$ensbl[[1]]$train_data)
